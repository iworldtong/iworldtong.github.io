---
layout: post
title: "R-C3D.pytorch源码详解"
subtitle: ''
author: "iworld"
header-img: img/2019-08-19-R-C3D.pytorch源码详解.png
mathjax: true
tags:
  - Deep Learning	
  - Video Understanding
---

本文结合[R-C3D](https://arxiv.org/abs/1703.07814)原理及其[Caffe实现](https://github.com/VisionLearningGroup/R-C3D)，介绍了[R-C3D.pytorch](https://github.com/sunnyxiaohu/R-C3D.pytorch)源代码的各部分功能。

### 0.目录

- [1.数据预处理](#1)
- [2.数据集加载](#2)
- [3.R-C3D框架](#3)



### <span id="1">1.数据预处理</span>

目标：滑动窗法采样数据，生成pkl文件。

在`preprocess`文件夹下，有分别对THUMOS14、ActivityNet和Charades三个数据集的预处理文件，文件名为`generate_roidb_training.py`和`generate_roidb_validation.py`。这里主要关注针对不同数据集，滑动窗的大小设置。预处理文件中有如下部分：

```python
# for ActivityNet
...
FPS = 25
...
WINS = [LENGTH * 8] 
...
```

论文根据不同数据集中行为片段的时长统计信息，设置对三个数据集采样的FPS分别为25、3和5。而一开始，分帧程序对所有视频的fps均为25（见`generate_frames.py`，这样比较通用，方便测试其他模型），因此需要进行一定转换。

具体的取帧部分在`lib/roi_data_layer/minibatch.py`中的`_get_video_blob`函数中（详见后文介绍），帧之间的采样间隔step等于预处理文件中的stride。

以ActivityNet为例，stride就是上面程序中的8。而25//8的采样率正好约为论文中的3fps，因此上面的WINS设置方式实际为` WINS = [LENGTH * FPS // sample_fps] `，其中sample_fps为真正的采样率，需要根据不同数据集进行设置。



### <span id="2">2.数据集加载</span>

主要涉及`lib/roi_data_layer`文件下的`roibatchLoader.py`和`minibatch.py`两个文件。

对于`roibatchLoader.py`，其返回数据为一段滑动窗对应数据（窗内采样图像序列和窗标签，其中标签为（max_num_box, x1, x2, cls），表示对当前滑动窗所有符合条件的ground truth boxes，也即回归分类目标）。若当前符合条件的gt boxes数量不足max_num_box，则用0填充；多余则截断。

对于`minibatch.py`，其输入为长度为1的滑动窗列表，即"Single batch only"，其中的random_scale_inds参数也只是选择一种窗口长度（`len(WINS)==1`）。

在具体取帧时，`_get_video_blob`函数中设置了不同的采样方式（稀疏或均匀抽帧）：

```python
...
if cfg.TEMP_SPARSE_SAMPLING:       
  if phase == 'train':
    segment_offsets = npr.randint(step, size=len(range(video_info[1], video_info[2], step)))
  else:
    segment_offsets = np.zeros(len(range(video_info[1], video_info[2], step))) + step // 2
else:            
  segment_offsets = np.zeros(len(range(video_info[1], video_info[2], step)))
...
```

对每个滑动窗，对其framestamps以sample_fps进行采样，并添加偏移segment_offsets，由于一段滑动窗长度为`win = LENGTH * FPS // sample_fps`，抽帧的间隔为`step = FPS // sample_fps`，则输出的video大小为`(LENGTH, cfg.TRAIN.CROP_SIZE, cfg.TRAIN.CROP_SIZE, 3)`，若采得总帧数不足`LENGTH`（窗宽小于`LENGTH`，代表此时滑动窗口溢出视频边界）则以最后一帧向后复制。

最终返回的data大小为`(1, channel, video_length, height, width)`。



### <span id="3">3.R-C3D框架</span>

以C3D为例。

R-C3D网络的初始化代码如下：

```python
	# initilize the network here.
    if args.net == 'c3d':
        tdcnn_demo = c3d_tdcnn(pretrained=True)
    elif args.net == 'res18':
        tdcnn_demo = resnet_tdcnn(depth=18, pretrained=True)
    elif args.net == 'res34':
        tdcnn_demo = resnet_tdcnn(depth=34, pretrained=True)
    elif args.net == 'res50':
        tdcnn_demo = resnet_tdcnn(depth=50, pretrained=True)
    elif args.net == 'eco':
        tdcnn_demo = eco_tdcnn(pretrained=True)        
    else:
        print("network is not defined")

    tdcnn_demo.create_architecture() # 初始化模型以及权重，见 lib/model/tdcnn/tdcnn.py
```

c3d_tdcnn继承自_TDCNN，主要作用是在tdcnn的基础上定义了下面四部分：

* **self.RCNN_base**：对输入视频提取特征图，取C3D base到conv5b为止（不含max pooling，C3D网络定义见`lib/model/tdcnn/c3d.py`），当输入大小为$(bsz, C, L, H, W)$时，对应的输出大小为$(bsz,512,L/8,H/16,W/16)$。
* **self.RCNN_top**：为C3D classifier，只取了第一个FC层（即fc6，包括ReLU与Dropout），输出为4096。
* **self.RCNN_cls_score**：输入4096维特征向量，得到分类得分。
* **self.RCNN_twin_pred**：输入4096维特征向量，得到每类的检测边界。

_TDCNN主要定义了rpn的相关结构：

* **self.RCNN_rpn**
  定义见`lib/model/rpn/rpn.py`，输入self.RCNN_base得到的特征图，输出rois, rpn_cls_prob, rpn_twin_pred, self.rpn_loss_cls, self.rpn_loss_twin, self.rpn_label, self.rpn_loss_mask。

  假设输入特征图大小为$(bsz, 512, 96, 7, 7)$，首先经过self.RPN_Conv1、self.RPN_Conv2和self.RPN_output_pool，输出rpn_output_pool大小为$(bsz, 512, 96, 1, 1)$，再进行后续处理。

  * **self.RPN_cls_score**：计算分类得分，对每一个time anchor（这里为96），有$2(bg/fg) \times 10 (anchors)$个输出，因此$(bsz, 512, 96, 1, 1) \rightarrow (bsz, 2*10, 96, 1, 1)$。

  * **self.RPN_twin_pred**：计算坐标，有$2(coords）\times 10 (anchors)$个输出，因此$(bsz, 512, 96, 1, 1) \rightarrow (bsz, 2*10, 96, 1, 1)$。

  * **self.RPN_proposal**

    通过前面得到的rpn_cls_prob和twin_pred，计算rois。

  * **self.RPN_anchor_target**

    训练阶段使用，得到self.rpn_label，self.rpn_loss_cls，self.rpn_loss_twin。

* **self.RCNN_proposal_target**

  定义见`lib/model/rpn/proposal_target_layer_cascade.py`，用于训练阶段。

* **self.RCNN_roi_temporal_pool**

* **self.RCNN_attention**

R-C3D前向计算的主要代码如下：

```python
feats_map = RCNN_base(input)
rois, rpn_loss_cls, rpn_loss_twin = RCNN_rpn(feats_map)

# if it is training phase, then use ground truth twins for refining
if is_training:
  rois, rois_label, rois_target, rois_inside_ws, rois_outside_ws = RCNN_proposal_target(rois, gt_twins)

# do roi pooling based on predicted rois
if is_pool:
  pooled_feat = RCNN_roi_temporal_pool(base_feat, rois.view(-1,3))
  
if USE_ATTENTION:
  pooled_feat = RCNN_attention(pooled_feat) 
  
pooled_feat = flatten(pooled_feat)
pooled_feat = RCNN_top(pooled_feat)

cls_score = RCNN_cls_score(pooled_feat)
cls_prob = softmax(cls_score, dim=1)

twin_pred = RCNN_twin_pred(pooled_feat)

if is_training:
  compute RCNN_loss_cls, RCNN_loss_twin according to twin_pred

if training:        
  return rois, cls_prob, twin_pred, rpn_loss_cls, rpn_loss_twin, RCNN_loss_cls, RCNN_loss_twin, rois_label
else:
  return rois, cls_prob, twin_pred 
```

